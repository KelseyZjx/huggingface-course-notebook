{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b5pJRchqbKL",
        "outputId": "b34f18fb-99bb-416d-80c2-dcbd2f718807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.5)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.25.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting git-lfs\n",
            "  Downloading git_lfs-1.6-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Downloading git_lfs-1.6-py2.py3-none-any.whl (5.6 kB)\n",
            "Installing collected packages: git-lfs\n",
            "Successfully installed git-lfs-1.6\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install git-lfs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#setup git\n",
        "!git config --global user.email \"you@example.com\"\n",
        "!git config --global user.name \"Your Name\""
      ],
      "metadata": {
        "id": "1kQmFimDsZ38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#登录hugging face hub\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "dpdnEBV1szGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "#TrainingArguments用于定义训练过程中的设置\n",
        "#bert-finetuned-mrpc是训练保存的输出目录的路径\n",
        "#每个epoch结束时保存一次\n",
        "#训练结束后自动将模型推送到hugging face model hub\n",
        "training_args = TrainingArguments(\n",
        "    \"bert-finetuned-mrpc\", save_strategy=\"epoch\", push_to_hub=True\n",
        ")"
      ],
      "metadata": {
        "id": "FxN2kRmftJq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Camembert是用于Masked Language Modeling（MLM）任务的预训练模型\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "checkpoint = \"camembert-base\"\n",
        "model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "_dYyQ06VuKyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#调用push_to_hub方法将模型和分词器上传到hugging face hub上进行共享\n",
        "\n",
        "#将名为dummy-model的model上传到hub\n",
        "model.push_to_hub(\"dummy-model\")\n",
        "#上传tokenizer\n",
        "tokenizer.push_to_hub(\"dummy-model\")\n",
        "#上传tokenizer并指定它属于一个组织——hugging face\n",
        "tokenizer.push_to_hub(\"dummy-model\", organization=\"huggingface\")\n",
        "#上传tokenizer,指定组织，还使用了认证令牌（token）\n",
        "# <token>需要被替换成实际的认证令牌，该令牌用于验证上传者的身份\n",
        "#hugging face需要认证令牌来确保只有授权用户才能上传或管理模型\n",
        "tokenizer.push_to_hub(\"dummy-model\", organization=\"huggingface\", use_auth_token=\"<TOKEN>\")"
      ],
      "metadata": {
        "id": "kr0fcjQlxRmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#导入hugging face中的一些功能\n",
        "\n",
        "from huggingface_hub import(\n",
        "    #User management\n",
        "    login,\n",
        "    logout,\n",
        "    whoami, #返回当前已登录用户的账户信息，包括用户名和与该账户相关的权限\n",
        "\n",
        "    #Repository creation and management\n",
        "    create_repo,\n",
        "    delete_repo,\n",
        "    update_repo_visibility, #用于更新仓库的可见性设置。可设置为public或private\n",
        "\n",
        "    #Some methods to retrieve/change information about the content\n",
        "    list_models,\n",
        "    list_datasets,\n",
        "    list_metrics,\n",
        "    list_repo_files,\n",
        "    upload_file,\n",
        "    delete_file,\n",
        ")\n"
      ],
      "metadata": {
        "id": "Zv9tQALRzI8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#创建仓库\n",
        "from huggingface_hub import create_repo\n",
        "\n",
        "create_repo(\"dummy-model\")"
      ],
      "metadata": {
        "id": "QNrWMwGe0txH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#创建仓库并指定组织\n",
        "from huggingface_hub import create_repo\n",
        "\n",
        "create_repo(\"dummy-model\", prganication=\"huggingface\")"
      ],
      "metadata": {
        "id": "hf8-nm5k0t1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#使用upload_file函数将本地文件上传到hugging face hub上的指定仓库\n",
        "from huggingface_hub import upload_file\n",
        "\n",
        "upload_file(\n",
        "    \"<path_to_file>/config.json\", #希望上传的本地文件的路径\n",
        "    path_in_repo=\"config.json\", #文件上传到hugging face hub后仓库中的存储路径或文件名\n",
        "    repo_id=\"<namespace>/dummy-model\" #仓库标识符。例如\"my-username/dummy-model\"唯一标识了dummy-model仓库，属于用户my-username\n",
        ")"
      ],
      "metadata": {
        "id": "vCrCC1te1fvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#使用Repository将指定仓库从Huggingface hub克隆到本地目录，或将本地目录与远程仓库进行关联\n",
        "repo = Repository(\"<path_to_dummy_folder>\", clone_from=\"<namespace>/dummy-model\")\n",
        "#<path_to_dummy_folder>是希望在本地创建或克隆仓库的目录路径"
      ],
      "metadata": {
        "id": "3orc7HxF3x2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Git操作方法，通常用于在本地和hugging face hub仓库之间同步更改\n",
        "\n",
        "repo.git_pull() #从远程仓库拉取最新的更新到本地仓库\n",
        "repo.git_add() #将本地更改添加到暂存区，这样，我的更改就准备好被提交了\n",
        "repo.git_commit() #提交本地的暂存区更改，并添加提交信息，更改的是本地仓库\n",
        "repo.git_push() #将本地的提交推送到远程仓库，更改的是远程仓库\n",
        "repo.git_tag() #给当前的提交创建一个标签"
      ],
      "metadata": {
        "id": "ZKpJ1wJO4gh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#将模型和分词器的状态保存到本地文件夹\n",
        "model.save_pretrained(\"<path_to_dummy_folder>\")\n",
        "tokenizer.save_pretrained(\"<path_to_dummy_folder>\")"
      ],
      "metadata": {
        "id": "e15ocTE15852"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "从 Hugging Face Hub 加载一个预训练的 Camembert 模型和分词器。\n",
        "对模型进行操作（训练、微调等）。\n",
        "将训练后的模型和分词器保存到指定的本地文件夹，以便后续使用。\n",
        "\n",
        "完整的过程如下：\n",
        "\"\"\"\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "checkpoint = \"camembert-base\"\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Do whatever with the model, train it, fine-tune it...\n",
        "\n",
        "model.save_pretrained(\"<path_to_dummy_folder>\")\n",
        "tokenizer.save_pretrained(\"<path_to_dummy_folder>\")"
      ],
      "metadata": {
        "id": "58VKbcla7QJk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}